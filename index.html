<!DOCTYPE HTML>
<!--
	Stellar by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>2024 seminar</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header" class="alt">
						<!-- <span class="logo"><img src="images/logo.svg" alt="" /></span> -->
						<h1>2024 seminar LIA</h1>
						<p>Experiments on Latent Image Animator<br />
						built by Zuo Zhirui, Xiang Hui and Xiong Ziwen for 2024 Seminar</a>.</p>
					</header>

				<!-- Nav -->
					<nav id="nav">
						<ul>
							<!-- <li><a href="#intro" class="active">Introduction</a></li> -->
							<li><a href="#first">Dataset</a></li>
							<li><a href="#second">Experiments</a></li>
							<li><a href="#cta">Results</a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">
						<!-- Introduction -->
							<!-- <section id="intro" class="main">
								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>Introduction</h2>
										</header>
										<p>Sed lorem ipsum dolor sit amet nullam consequat feugiat consequat magna
										adipiscing magna etiam amet veroeros. Lorem ipsum dolor tempus sit cursus.
										Tempus nisl et nullam lorem ipsum dolor sit amet aliquam.  
										build neng tong bu m?
										</p>
										<ul class="actions">
											<li><a href="generic.html" class="button">Learn More</a></li>
										</ul>
									</div>
									<span class="image"><img src="images/pic01.jpg" alt="" /></span>
								</div>
							</section> -->

						<!-- First Section -->
							<section id="first" class="main special">
								<header class="major">
									<h2>Dataset</h2>
								</header>
								<ul class="features">
									<li>
										<!-- <span class="icon solid major style1 fa-code"></span> -->
										<h3>Pretrained models</h3>
										<p>We used the open-source model taichi.pt provided by Latent Image Animator as the pretrained model.</p>
									</li>
									<li>
										<!-- <span class="icon major style3 fa-copy"></span> -->
										<h3>Dataset</h3>
										<p>We used the UBC Fashion dataset proposed in DwNet for finetuning, which includes 500 training videos and 100 testing videos.</p>
									</li>
								</ul>
								<video width="320" height="240" controls>
									<source src="images/video.mp4" type="video/mp4">
								  </video>
								<footer class="major">
									<!-- <ul class="actions special">
										<li><a href="generic.html" class="button">Learn More</a></li>
									</ul> -->
								</footer>
							</section>
						<!-- Second Section -->
							<section id="second" class="main special">
								<header class="major">
									<h2>Experiments</h2>
									<p>Due to the lengthy pretraining , we finetuned the open-source model and achieved good results. <br>Based on this, we conducted three attempts at the finetuning process.<br />
									</p>
								</header>

								<h2>Finetune with different loss</h2>

								<p>xxx</p>

								<h2>Finetune with adapter</h2>

								<p>xxx</p>

								<h2>Finetune with LoRA</h2>

								<p>Low Rank Adaptation (LoRA) is used to address the challenges faced in finetuning large language models. For large models with billions of parameters, finetuning for specific domains is costly. LoRA retains the weights of pretrained models and adds trainable layers within each model block. This leads to a significant reduction in the number of parameters that need to be finetuned. Generally speaking, LoRA is used in transformer architectures. However, LIA is composed of convolutional neural networks, so we use ConvLoRA from the article <i> CnvLoRA and AdaBN Based Domain Adaptation via Self Training </i> for finetuning.</p>
								<img src="images/convlora.png" width="600" height="300">
								<!-- <footer class="major">
									<ul class="actions special">
										<li><a href="generic.html" class="button">Learn More</a></li>
									</ul>
								</footer> -->
							</section>
						<!-- Get Started -->
							<section id="cta" class="main special">
								<header class="major">
									<h2>Results</h2>
									<p>Here are the results of the above three attempts.</p>
								</header>
								<h2>Finetune with different loss</h2>

								<p>xxx</p>

								<h2>Finetune with adapter</h2>

								<p>xxx</p>

								<h2>Finetune with LoRA</h2>
								<table>
									<thead>
									  <tr>
										<th>Model</th>
										<th>Recon loss</th>
										<th>Lpips loss</th>
									  </tr>
									</thead>
									<tbody>
									  <tr>
										<td>行1，列1</td>
										<td>行1，列2</td>
										<td>行1，列3</td>
									  </tr>
									  <tr>
										<td>行2，列1</td>
										<td>行2，列2</td>
										<td>行2，列3</td>
									  </tr>
									</tbody>
								  </table>

								<p class="content">Nam elementum </p>
								<!-- <footer class="major">
									<ul class="actions special">
										<li><a href="generic.html" class="button primary">Get Started</a></li>
										<li><a href="generic.html" class="button">Learn More</a></li>
									</ul>
								</footer> -->
							</section>

					</div>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
