<!DOCTYPE HTML>
<!--
	Stellar by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>2024 seminar</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header" class="alt">
						<!-- <span class="logo"><img src="images/logo.svg" alt="" /></span> -->
						<h1>2024 seminar LIA</h1>
						<p>Experiments on Latent Image Animator<br />
						built by Zuo Zhirui, Xiang Hui and Xiong Ziwen for 2024 Seminar</a>.</p>
					</header>

				<!-- Nav -->
					<nav id="nav">
						<ul>
							<!-- <li><a href="#intro" class="active">Introduction</a></li> -->
							<li><a href="#first">Dataset</a></li>
							<li><a href="#second">Experiments</a></li>
							<li><a href="#cta">Results</a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">
						<!-- Introduction -->
							<!-- <section id="intro" class="main">
								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>Introduction</h2>
										</header>
										<p>Sed lorem ipsum dolor sit amet nullam consequat feugiat consequat magna
										adipiscing magna etiam amet veroeros. Lorem ipsum dolor tempus sit cursus.
										Tempus nisl et nullam lorem ipsum dolor sit amet aliquam.  
										build neng tong bu m?
										</p>
										<ul class="actions">
											<li><a href="generic.html" class="button">Learn More</a></li>
										</ul>
									</div>
									<span class="image"><img src="images/pic01.jpg" alt="" /></span>
								</div>
							</section> -->

						<!-- First Section -->
							<section id="first" class="main special">
								<header class="major">
									<h2>Dataset</h2>
								</header>
								<ul class="features">
									<li>
										<!-- <span class="icon solid major style1 fa-code"></span> -->
										<h3>Pretrained models</h3>
										<p>We used the open-source model taichi.pt provided by Latent Image Animator as the pretrained model.</p>
									</li>
									<li>
										<!-- <span class="icon major style3 fa-copy"></span> -->
										<h3>Dataset</h3>
										<p>We used the UBC Fashion dataset proposed in DwNet for finetuning, which includes 500 training videos and 100 testing videos.</p>
									</li>
								</ul>
								<video width="320" height="240" controls>
									<source src="images/video.mp4" type="video/mp4">
								  </video>
								<footer class="major">
									<!-- <ul class="actions special">
										<li><a href="generic.html" class="button">Learn More</a></li>
									</ul> -->
								</footer>
							</section>
						<!-- Second Section -->
							<section id="second" class="main special">
								<header class="major">
									<h2>Experiments</h2>
									<p>Due to the lengthy pretraining , we finetuned the open-source model and achieved good results. <br>Based on this, we conducted three attempts at the finetuning process.<br />
									</p>
								</header>

								<h2>Finetune with different loss</h2>

								<p>In the original work, the reconstruction loss used  L1 loss. In the fine-tuning experiments, we utilized pre-trained taichi model and added the SSIM loss to the reconstruction loss, specifically, we designed two reconstruction loss functions. The two reconstruction loss functions are shown in the figure below.</p>
								<img src="images/loss.png" width="440" height="110">

								<h2>Finetune with adapter</h2>

								<p>In the fine-tuning task, the optimization of a specific task can be carried out by introducing an adapter module. The adapter module is a lightweight set of parameters that are added to the middle layer of the model to protect the parameters of the original pre-trained model. The goal of this approach is to adapt the parameters of the adapter module to the new task without changing the overall model structure.In the experiment, we introduced the adapter into the model by way of residuals.</p>
								<img src="images/adapter.png" width="600" height="300">

								<h2>Finetune with LoRA</h2>

								<p>Low Rank Adaptation (LoRA) is used to address the challenges faced in finetuning large language models. For large models with billions of parameters, finetuning for specific domains is costly. LoRA retains the weights of pretrained models and adds trainable layers within each model block. This leads to a significant reduction in the number of parameters that need to be finetuned. Generally speaking, LoRA is used in transformer architectures. However, LIA is composed of convolutional neural networks, so we use ConvLoRA from the article <i> CnvLoRA and AdaBN Based Domain Adaptation via Self Training </i> for finetuning.</p>
								<img src="images/convlora.png" width="600" height="300">
								<!-- <footer class="major">
									<ul class="actions special">
										<li><a href="generic.html" class="button">Learn More</a></li>
									</ul>
								</footer> -->
							</section>
						<!-- Get Started -->
							<section id="cta" class="main special">
								<header class="major">
									<h2>Results</h2>
									<p>Here are the results of the above three attempts.</p>
								</header>
								<h2>Finetune with different loss</h2>
								<table>
									<thead>
									  <tr>
										<th>Model</th>
										<th>Recon loss</th>
										<th>Lpips loss</th>
									  </tr>
									</thead>
									<tbody>
									  <tr>
										<td>taichi_40000</td>
										<td>0.017870907</td>
										<td>0.047854178</td>
									  </tr>
									  <tr>
										<td>taichi_80000</td>
										<td>0.017218325</td>
										<td>0.04689284</td>
									  </tr>
									  <tr>
										<td>taichi_120000</td>
										<td>0.017635012</td>
										<td>0.04753318</td>
									  </tr>
									</tbody>
								  </table>

								<h2>Finetune with adapter</h2>
								<table>
									<thead>
									  <tr>
										<th>Model</th>
										<th>Recon loss</th>
										<th>Lpips loss</th>
									  </tr>
									</thead>
									<tbody>
									  <tr>
										<td>Adapter_40000</td>
										<td>0.021015551</td>
										<td>0.061745953</td>
									  </tr>
									  <tr>
										<td>Adapter_80000</td>
										<td>0.021210136</td>
										<td>0.06158965</td>
									  </tr>
									  <tr>
										<td>Adapter_12000</td>
										<td>0.025388759</td>
										<td>0.0610452</td>
									  </tr>
									</tbody>
								  </table>
								<p>Due to the extremely short time required to update the parameters of the discriminator, we provide the result that the discriminator does not use adapter.</p>

								<h2>Finetune with LoRA</h2>
								<table>
									<thead>
									  <tr>
										<th>Model</th>
										<th>Recon loss</th>
										<th>Lpips loss</th>
									  </tr>
									</thead>
									<tbody>
									  <tr>
										<td>Lora_40000</td>
										<td>0.019903766</td>
										<td>0.058042873</td>
									  </tr>
									  <tr>
										<td>Lora_80000</td>
										<td>0.019482996</td>
										<td>0.05589702</td>
									  </tr>
									  <tr>
										<td>Lora_120000</td>
										<td>0.01968369</td>
										<td>0.05461842</td>
									  </tr>
									  <tr>
										<td>Lora_w/o_Dis_40000</td>
										<td>0.020124692</td>
										<td>0.0595114</td>
									  </tr>
									  <tr>
										<td>Lora_w/o_Dis_80000</td>
										<td>0.019737989</td>
										<td>0.05686592</td>
									  </tr>
									  <tr>
										<td>Lora_w/o_Dis_120000</td>
										<td>0.019538457</td>
										<td> 0.05562961</td>
									  </tr>
									</tbody>
								  </table>

								<p>Due to the extremely short time required to update the parameters of the discriminator, we provide the result that the discriminator does not use ConvLoRA.</p>
								<!-- <footer class="major">
									<ul class="actions special">
										<li><a href="generic.html" class="button primary">Get Started</a></li>
										<li><a href="generic.html" class="button">Learn More</a></li>
									</ul>
								</footer> -->
							</section>

					</div>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
